1.什么是反向传播中的链式法则?
  答：反向传播时，将输出层的误差反向向隐藏层一层层的传播，在计算每一步的误差时，需要乘上上一步得到的误差，即反向传播时需要对误差层层相乘，这就是反向传播中的链式法则。

2.请列举几种常见的激活函数，激活函数有什么作用？
  答：常见的激活函数有：Sigmoid函数，tanh函数，Relu函数及其改进型函数（Leaky-ReLU、P-ReLU、R-ReLU），ELU (Exponential Linear Units) 函数，MaxOut函数等。
      激活函数主要解决的是神经网络非线性的问题。如果不用激活函数，则每一层节点的输入都是上层输出的线性函数，无论神经网络有多少层，输出都是输入的线性组合，网络的逼近能力相当有限。
而引入非线性函数作为激活函数后，神经网络就不再是输入的线性组合，而是几乎可以逼近任意函数。

3.利用梯度下降法训练神经网络，发现模型loss不变，可能有哪些问题？怎么解决？
  答：可能是神经网络随着层数的加深，梯度后向传播至浅层网络时，已经无法引起参数的扰动，没有办法将loss信息传递至浅层网络，权值也无法更新。解决办法有：一是使用其它激活函数；二是优化权重初始化方式；
三是层归一化等。

